{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c266505b",
   "metadata": {},
   "source": [
    "# Using same base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53465714",
   "metadata": {},
   "source": [
    "## Verify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f83fbbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data structure :\n",
      "\n",
      "TRAIN:\n",
      "  RGB: 600 files\n",
      "  MS: 600 files\n",
      "  HS: 600 files\n",
      "\n",
      "VAL:\n",
      "  RGB: 300 files\n",
      "  MS: 300 files\n",
      "  HS: 300 files\n",
      "\n",
      " sample filenames:\n",
      "\n",
      "train/RGB: ['Rust_hyper_181.png', 'Rust_hyper_195.png', 'Other_hyper_117.png', 'Other_hyper_103.png', 'Rust_hyper_142.png']\n",
      "\n",
      "val/HS: ['val_f720cb3d.tif', 'val_9eb4f156.tif', 'val_cc2ca03e.tif', 'val_8e9e3f20.tif', 'val_47de91ce.tif']\n",
      "\n",
      " train labels: {'Rust': 200, 'Other': 200, 'Health': 200} (total files: 600)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "DATA_ROOT = \"/Users/apple/Downloads/data\"\n",
    "TRAIN_DIR = os.path.join(DATA_ROOT, \"train\")\n",
    "VAL_DIR = os.path.join(DATA_ROOT, \"val\")\n",
    "\n",
    "print(\"data structure :\")\n",
    "for split, path in [(\"train\", TRAIN_DIR), (\"val\", VAL_DIR)]:\n",
    "    print(f\"\\n{split.upper()}:\")\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"{path} NON EXISTENT\")\n",
    "        continue\n",
    "    \n",
    "    for modality in [\"RGB\", \"MS\", \"HS\"]:\n",
    "        mod_path = os.path.join(path, modality)\n",
    "        if os.path.exists(mod_path):\n",
    "            count = len([f for f in os.listdir(mod_path) if f.lower().endswith((\".png\", \".tif\", \".tiff\"))])\n",
    "            print(f\"  {modality}: {count} files\")\n",
    "        else:\n",
    "            print(f\"  {modality}: MISSING\")\n",
    "\n",
    "# Sample filenames\n",
    "print(\"\\n sample filenames:\")\n",
    "for split, path in [(\"train/RGB\", os.path.join(TRAIN_DIR, \"RGB\")), (\"val/HS\", os.path.join(VAL_DIR, \"HS\"))]:\n",
    "    if os.path.exists(path):\n",
    "        files = [f for f in os.listdir(path)[:5] if f.lower().endswith((\".png\", \".tif\"))]\n",
    "        print(f\"\\n{split}: {files}\")\n",
    "    else:\n",
    "        print(f\"\\n{split}: PATH MISSING\")\n",
    "\n",
    "# Train label parsing check\n",
    "train_rgb_path = os.path.join(TRAIN_DIR, \"RGB\")\n",
    "if os.path.exists(train_rgb_path):\n",
    "    train_rgb_files = [f for f in os.listdir(train_rgb_path) if f.lower().endswith(\".png\")]\n",
    "    labels = [f.split(\"_\")[0] for f in train_rgb_files if \"_\" in f]\n",
    "    label_counts = Counter(labels)\n",
    "    print(f\"\\n train labels: {dict(label_counts)} (total files: {len(train_rgb_files)})\")\n",
    "else:\n",
    "    print(\"\\n !!! train/RGB missing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c537c37",
   "metadata": {},
   "source": [
    "## Install dependencies + adapt baseline CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d5061be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['uv', 'add', 'timm', 'tifffile', 'opencv-python', 'torch', 'torchvision'], returncode=0, stdout=b'', stderr=b'\\x1b[2mResolved \\x1b[1m94 packages\\x1b[0m \\x1b[2min 30ms\\x1b[0m\\x1b[0m\\n\\x1b[2mUninstalled \\x1b[1m1 package\\x1b[0m \\x1b[2min 1.62s\\x1b[0m\\x1b[0m\\n\\x1b[2mInstalled \\x1b[1m1 package\\x1b[0m \\x1b[2min 333ms\\x1b[0m\\x1b[0m\\n \\x1b[33m~\\x1b[39m \\x1b[1mtorch\\x1b[0m\\x1b[2m==2.10.0\\x1b[0m\\n')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.run([\"uv\", \"add\", \"timm\", \"tifffile\", \"opencv-python\", \"torch\", \"torchvision\"], capture_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "130e601e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG ready:\n",
      "  ROOT: /Users/apple/Downloads/data\n",
      "  RGB backbone: convnext_base\n"
     ]
    }
   ],
   "source": [
    "# setup cfg\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class CFG:\n",
    "    ROOT: str = \"/Users/apple/Downloads/data\"  \n",
    "    TRAIN_DIR: str = \"train\"\n",
    "    VAL_DIR: str = \"val\"\n",
    "    \n",
    "    USE_RGB: bool = True\n",
    "    USE_MS: bool = True  \n",
    "    USE_HS: bool = True\n",
    "    \n",
    "    IMG_SIZE: int = 224\n",
    "    BATCH_SIZE: int = 16  # smaller for local debugging\n",
    "    EPOCHS: int = 5       # few for testing\n",
    "    LR: float = 3e-4\n",
    "    WD: float = 1e-4\n",
    "    \n",
    "    NUM_WORKERS: int = 4\n",
    "    SEED: int = 3557\n",
    "    \n",
    "    RGB_BACKBONE: str = \"convnext_base\"  # swap to resnet later\n",
    "    AMP: bool = True\n",
    "    \n",
    "    HS_DROP_FIRST: int = 10\n",
    "    HS_DROP_LAST: int = 14\n",
    "    \n",
    "    OUT_DIR: str = \"./outputs\"\n",
    "    BEST_CKPT: str = \"best.pt\"\n",
    "\n",
    "cfg = CFG()\n",
    "print(\"CFG ready:\")\n",
    "print(f\"  ROOT: {cfg.ROOT}\")\n",
    "print(f\"  RGB backbone: {cfg.RGB_BACKBONE}\")\n",
    "os.makedirs(cfg.OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b73377",
   "metadata": {},
   "source": [
    "## Indexing + df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b627319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from typing import Dict, List\n",
    "from collections import defaultdict\n",
    "\n",
    "LABELS = [\"Health\", \"Rust\", \"Other\"]\n",
    "LBL2ID = {k: i for i, k in enumerate(LABELS)}\n",
    "ID2LBL = {i: k for k, i in LBL2ID.items()}\n",
    "\n",
    "def list_files(folder: str, exts: tuple) -> List[str]:\n",
    "    if not os.path.isdir(folder):\n",
    "        return []\n",
    "    return sorted([os.path.join(folder, fn) \n",
    "                  for fn in os.listdir(folder) \n",
    "                  if fn.lower().endswith(exts)])\n",
    "\n",
    "def base_id(path: str) -> str:\n",
    "    return os.path.splitext(os.path.basename(path))[0]\n",
    "\n",
    "def parse_label_from_train_name(bid: str) -> str:\n",
    "    m = re.match(r\"^(Health|Rust|Other)_\", bid)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def build_index(root: str, split: str) -> Dict[str, Dict[str, str]]:\n",
    "    split_dir = os.path.join(root, split)\n",
    "    rgb_dir = os.path.join(split_dir, \"RGB\")\n",
    "    ms_dir  = os.path.join(split_dir, \"MS\") \n",
    "    hs_dir  = os.path.join(split_dir, \"HS\")\n",
    "    \n",
    "    rgb_files = list_files(rgb_dir, (\".png\",))\n",
    "    ms_files  = list_files(ms_dir,  (\".tif\", \".tiff\"))\n",
    "    hs_files  = list_files(hs_dir,  (\".tif\", \".tiff\"))\n",
    "    \n",
    "    idx = {}\n",
    "    for p in rgb_files: idx.setdefault(base_id(p), {})[\"rgb\"] = p\n",
    "    for p in ms_files:  idx.setdefault(base_id(p), {})[\"ms\"]  = p  \n",
    "    for p in hs_files:  idx.setdefault(base_id(p), {})[\"hs\"]  = p\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3f61257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train IDs: 600\n",
      "Val IDs:   300\n",
      "\n",
      "Sample alignment: 0/10 missing modalities\n"
     ]
    }
   ],
   "source": [
    "# build index\n",
    "train_idx = build_index(cfg.ROOT, cfg.TRAIN_DIR)\n",
    "val_idx   = build_index(cfg.ROOT, cfg.VAL_DIR)\n",
    "\n",
    "print(f\"Train IDs: {len(train_idx)}\")\n",
    "print(f\"Val IDs:   {len(val_idx)}\")\n",
    "\n",
    "# check alignment\n",
    "missing_modalities = []\n",
    "for bid, paths in list(train_idx.items())[:10]:  # sample\n",
    "    missing = [m for m in [\"rgb\",\"ms\",\"hs\"] if m not in paths]\n",
    "    if missing: missing_modalities.append((bid, missing))\n",
    "\n",
    "print(f\"\\nSample alignment: {len(missing_modalities)}/10 missing modalities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2aa891",
   "metadata": {},
   "source": [
    "## Hold-out split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04bb8c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# subprocess.run([\"uv\", \"add\", \"pandas\", \"numpy\"], capture_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d90c5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build train_df/val_df + stratified holdout\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def make_train_df(train_idx: Dict[str, Dict[str, str]]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for bid, paths in train_idx.items():\n",
    "        lab = parse_label_from_train_name(bid)\n",
    "        if lab:\n",
    "            rows.append({\"base_id\": bid, \"label\": lab, \n",
    "                        \"rgb\": paths.get(\"rgb\"), \"ms\": paths.get(\"ms\"), \"hs\": paths.get(\"hs\")})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def make_val_df(val_idx: Dict[str, Dict[str, str]]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for bid, paths in val_idx.items():\n",
    "        rows.append({\"base_id\": bid, \"rgb\": paths.get(\"rgb\"), \n",
    "                    \"ms\": paths.get(\"ms\"), \"hs\": paths.get(\"hs\")})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def stratified_holdout(df: pd.DataFrame, frac: float = 0.1, seed: int = 42) -> tuple:\n",
    "    df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    parts = []\n",
    "    for lab, g in df.groupby(\"label\"):\n",
    "        n = max(1, int(len(g) * frac))\n",
    "        parts.append(g.iloc[:n])\n",
    "    df_va = pd.concat(parts).drop_duplicates(\"base_id\")\n",
    "    df_tr = df[~df[\"base_id\"].isin(df_va[\"base_id\"])].reset_index(drop=True)\n",
    "    return df_tr, df_va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d8c84a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DF: 600 rows; label dist: {'Health': 200, 'Other': 200, 'Rust': 200}\n",
      "Val DF:   300 rows\n",
      "\n",
      "SPLITS:\n",
      "  Training:   540\n",
      "  Holdout VA: 60\n",
      "\n",
      "SAMPLE TRAIN ROW:\n",
      "         base_id  label\n",
      "Health_hyper_146 Health\n",
      "Health_hyper_200 Health\n"
     ]
    }
   ],
   "source": [
    "train_df = make_train_df(train_idx)\n",
    "val_df   = make_val_df(val_idx)\n",
    "\n",
    "print(f\"Train DF: {len(train_df)} rows; label dist: {train_df['label'].value_counts().to_dict()}\")\n",
    "print(f\"Val DF:   {len(val_df)} rows\")\n",
    "\n",
    "# STRATIFIED HOLDOUT (10% of train for validation)\n",
    "df_tr, df_va_holdout = stratified_holdout(train_df, frac=0.1, seed=cfg.SEED)\n",
    "print(f\"\\nSPLITS:\")\n",
    "print(f\"  Training:   {len(df_tr)}\")\n",
    "print(f\"  Holdout VA: {len(df_va_holdout)}\")\n",
    "\n",
    "# Preview\n",
    "print(\"\\nSAMPLE TRAIN ROW:\")\n",
    "print(df_tr.head(2)[[\"base_id\", \"label\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c4ac49",
   "metadata": {},
   "source": [
    "## HS channel inf + preview dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ae7d57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tifffile as tiff\n",
    "\n",
    "def read_tiff_multiband(path: str) -> np.ndarray:\n",
    "    arr = tiff.imread(path)\n",
    "    if arr.ndim != 3:\n",
    "        raise ValueError(f\"Expected 3D TIFF, got {arr.shape}\")\n",
    "    if arr.shape[0] < arr.shape[1]:  # (C,H,W) -> (H,W,C)\n",
    "        arr = np.transpose(arr, (1, 2, 0))\n",
    "    return arr\n",
    "\n",
    "def read_hs(path: str, drop_first: int, drop_last: int) -> torch.Tensor:\n",
    "    arr = read_tiff_multiband(path)\n",
    "    B = arr.shape[2]\n",
    "    if B > (drop_first + drop_last + 1):\n",
    "        arr = arr[:, :, drop_first:B-drop_last]\n",
    "    # Min-max norm per band (simplified)\n",
    "    for c in range(arr.shape[2]):\n",
    "        arr[:,:,c] = (arr[:,:,c] - arr[:,:,c].min()) / (arr[:,:,c].max() - arr[:,:,c].min() + 1e-6)\n",
    "    return torch.from_numpy(arr).permute(2, 0, 1).float()\n",
    "\n",
    "def infer_hs_in_ch(train_df, val_df, cfg):\n",
    "    for df in (train_df, val_df):\n",
    "        hs_paths = df[\"hs\"].dropna().tolist()\n",
    "        if hs_paths:\n",
    "            x = read_hs(hs_paths[0], cfg.HS_DROP_FIRST, cfg.HS_DROP_LAST)\n",
    "            print(f\"HS sample shape: {x.shape} ({x.shape[0]} bands)\")\n",
    "            return int(x.shape[0])\n",
    "    return 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d80b23aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HS sample shape: torch.Size([101, 32, 32]) (101 bands)\n",
      "\n",
      "HS_IN_CH = 101\n",
      "\n",
      "Testing sample: Health_hyper_146\n",
      "  RGB: True\n",
      "  MS:  True\n",
      "  HS:  True\n"
     ]
    }
   ],
   "source": [
    "# INFER HS CHANNELS\n",
    "hs_in_ch = infer_hs_in_ch(train_df, val_df, cfg)\n",
    "print(f\"\\nHS_IN_CH = {hs_in_ch}\")\n",
    "\n",
    "# Test one full sample\n",
    "sample_row = df_tr.iloc[0]\n",
    "print(f\"\\nTesting sample: {sample_row['base_id']}\")\n",
    "print(f\"  RGB: {sample_row['rgb'] is not None}\")\n",
    "print(f\"  MS:  {sample_row['ms'] is not None}\")\n",
    "print(f\"  HS:  {sample_row['hs'] is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ba2e67",
   "metadata": {},
   "source": [
    "## Full dataset class + quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a3bb431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "\n",
    "def read_rgb(path: str) -> torch.Tensor:\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "    x = torch.from_numpy(img).permute(2, 0, 1)\n",
    "    return (x - IMAGENET_MEAN) / IMAGENET_STD\n",
    "\n",
    "def resize_tensor(x: torch.Tensor, size: int) -> torch.Tensor:\n",
    "    return F.interpolate(x.unsqueeze(0), size=(size, size), mode=\"bilinear\").squeeze(0)\n",
    "\n",
    "class WheatMultiModalDataset(Dataset):\n",
    "    def __init__(self, df, cfg, hs_in_ch, train=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.cfg = cfg\n",
    "        self.hs_in_ch = hs_in_ch\n",
    "        self.train = train\n",
    "    \n",
    "    def __len__(self): return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        \n",
    "        x_rgb = x_ms = x_hs = None\n",
    "        m_rgb = m_ms = m_hs = 0.0\n",
    "        \n",
    "        # RGB\n",
    "        if self.cfg.USE_RGB and row.get(\"rgb\"):\n",
    "            x_rgb = read_rgb(row[\"rgb\"])\n",
    "            x_rgb = resize_tensor(x_rgb, self.cfg.IMG_SIZE)\n",
    "            m_rgb = 1.0\n",
    "        \n",
    "        # MS (5 bands)\n",
    "        if self.cfg.USE_MS and row.get(\"ms\"):\n",
    "            arr = read_tiff_multiband(row[\"ms\"])\n",
    "            arr = (arr - arr.min()) / (arr.max() - arr.min() + 1e-6)  # simple norm\n",
    "            x_ms = torch.from_numpy(arr).permute(2, 0, 1).float()\n",
    "            x_ms = resize_tensor(x_ms, self.cfg.IMG_SIZE)\n",
    "            m_ms = 1.0\n",
    "        \n",
    "        # HS\n",
    "        if self.cfg.USE_HS and row.get(\"hs\"):\n",
    "            x_hs = read_hs(row[\"hs\"], self.cfg.HS_DROP_FIRST, self.cfg.HS_DROP_LAST)\n",
    "            x_hs = resize_tensor(x_hs, self.cfg.IMG_SIZE)\n",
    "            m_hs = 1.0\n",
    "        \n",
    "        # Zero-pad missing\n",
    "        if x_rgb is None: x_rgb = torch.zeros(3, self.cfg.IMG_SIZE, self.cfg.IMG_SIZE)\n",
    "        if x_ms  is None: x_ms  = torch.zeros(5, self.cfg.IMG_SIZE, self.cfg.IMG_SIZE)\n",
    "        if x_hs  is None: x_hs  = torch.zeros(self.hs_in_ch, self.cfg.IMG_SIZE, self.cfg.IMG_SIZE)\n",
    "        \n",
    "        mask = torch.tensor([m_rgb, m_ms, m_hs])\n",
    "        \n",
    "        if \"label\" in row:\n",
    "            return {\"id\": row[\"base_id\"], \"rgb\": x_rgb, \"ms\": x_ms, \"hs\": x_hs, \n",
    "                   \"mask\": mask, \"y\": torch.tensor(LBL2ID[row[\"label\"]])}\n",
    "        return {\"id\": row[\"base_id\"], \"rgb\": x_rgb, \"ms\": x_ms, \"hs\": x_hs, \"mask\": mask}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b5785e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample shapes:\n",
      "  RGB: torch.Size([3, 224, 224])  mask: 1.0\n",
      "  MS:  torch.Size([5, 224, 224])  mask: 1.0\n",
      "  HS:  torch.Size([101, 224, 224])  mask: 1.0\n",
      "  Label: Health_hyper_146 -> Health\n"
     ]
    }
   ],
   "source": [
    "# TEST DATASET\n",
    "ds_tr = WheatMultiModalDataset(df_tr, cfg, hs_in_ch, train=True)\n",
    "sample = ds_tr[0]\n",
    "\n",
    "print(f\"Sample shapes:\")\n",
    "print(f\"  RGB: {sample['rgb'].shape}  mask: {sample['mask'][0]:.1f}\")\n",
    "print(f\"  MS:  {sample['ms'].shape}  mask: {sample['mask'][1]:.1f}\")\n",
    "print(f\"  HS:  {sample['hs'].shape}  mask: {sample['mask'][2]:.1f}\")\n",
    "print(f\"  Label: {sample['id']} -> {ID2LBL[sample['y'].item()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a51932e",
   "metadata": {},
   "source": [
    "## ConvNeXt baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc1339d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/Desktop/Kaggle_competion-Beyond-Visible-Spectrum-AI-for-Agriculture-2026/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "class SmallSpectralEncoder(nn.Module):\n",
    "    def __init__(self, in_ch: int, emb_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, 32, 1, bias=False),\n",
    "            nn.BatchNorm2d(32), nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding=1, bias=False), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1, bias=False), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding=1, bias=False), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1), nn.Flatten(), \n",
    "            nn.Linear(128, emb_dim), nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x): \n",
    "        x = self.stem(x)\n",
    "        x = self.block(x)\n",
    "        return self.head(x)\n",
    "\n",
    "class MultiModalNet(nn.Module):\n",
    "    def __init__(self, cfg, hs_in_ch, n_classes=3):\n",
    "        super().__init__()\n",
    "        self.use_rgb = cfg.USE_RGB\n",
    "        self.use_ms = cfg.USE_MS\n",
    "        self.use_hs = cfg.USE_HS\n",
    "        \n",
    "        feat_dims = []\n",
    "        if self.use_rgb:\n",
    "            self.rgb_enc = timm.create_model(cfg.RGB_BACKBONE, pretrained=True, num_classes=0, global_pool=\"avg\")\n",
    "            feat_dims.append(self.rgb_enc.num_features)\n",
    "        if self.use_ms: \n",
    "            self.ms_enc = SmallSpectralEncoder(5, 256)\n",
    "            feat_dims.append(256)\n",
    "        if self.use_hs:\n",
    "            self.hs_enc = SmallSpectralEncoder(hs_in_ch, 256) \n",
    "            feat_dims.append(256)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(sum(feat_dims), 512), nn.ReLU(inplace=True), nn.Dropout(0.2),\n",
    "            nn.Linear(512, n_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, rgb, ms, hs, mask):\n",
    "        feats = []\n",
    "        if self.use_rgb: feats.append(self.rgb_enc(rgb) * mask[:, 0:1])\n",
    "        if self.use_ms:  feats.append(self.ms_enc(ms)  * mask[:, 1:2])\n",
    "        if self.use_hs:  feats.append(self.hs_enc(hs)  * mask[:, 2:3])\n",
    "        return self.classifier(torch.cat(feats, dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7eb49586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created on cpu\n",
      "  RGB dim: 1024\n",
      "  Total params: 88,905,027\n"
     ]
    }
   ],
   "source": [
    "# instantiate\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultiModalNet(cfg, hs_in_ch).to(device)\n",
    "print(f\"Model created on {device}\")\n",
    "print(f\"  RGB dim: {model.rgb_enc.num_features if cfg.USE_RGB else 'N/A'}\")\n",
    "print(f\"  Total params: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fc9cce",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b946c1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated: RGB=resnet50, Batch=8\n",
      "Now using ResNet50 + raw HS (PCA next)\n",
      "  RGB dim: 2048\n"
     ]
    }
   ],
   "source": [
    "cfg.RGB_BACKBONE = \"resnet50\"  \n",
    "cfg.BATCH_SIZE = 8              # smaller for CPU lol\n",
    "print(f\"Updated: RGB={cfg.RGB_BACKBONE}, Batch={cfg.BATCH_SIZE}\")\n",
    "\n",
    "# RELOAD MODEL with ResNet\n",
    "model = MultiModalNet(cfg, hs_in_ch).to(device)\n",
    "print(f\"Now using ResNet50 + raw HS (PCA next)\")\n",
    "print(f\"  RGB dim: {model.rgb_enc.num_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abd6480",
   "metadata": {},
   "source": [
    "## PCA for hs dim reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f47820bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# subprocess.run([\"uv\", \"add\", \"scikit-learn\", \"joblib\"], capture_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7e99068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HS BAND COUNTS (first 20):\n",
      "Raw shapes: [(32, 32, 125), (32, 32, 126), (32, 32, 126), (32, 32, 125), (32, 32, 125)]\n",
      "Trimmed bands: [('Health_hyper_146.tif', (32, 32, 125), 101), ('Health_hyper_200.tif', (32, 32, 126), 102), ('Health_hyper_14.tif', (32, 32, 126), 102), ('Other_hyper_116.tif', (32, 32, 125), 101), ('Other_hyper_27.tif', (32, 32, 125), 101)]\n",
      "Unique trimmed B: [101, 102]\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: Check HS band counts\n",
    "hs_paths = df_tr[\"hs\"].dropna().tolist()[:20]  # first 20\n",
    "band_counts = []\n",
    "for path in hs_paths:\n",
    "    try:\n",
    "        arr = read_tiff_multiband(path)\n",
    "        B_trim = arr.shape[2] - cfg.HS_DROP_FIRST - cfg.HS_DROP_LAST\n",
    "        band_counts.append((os.path.basename(path), arr.shape, B_trim))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"HS BAND COUNTS (first 20):\")\n",
    "unique_bands = set(b[2] for b in band_counts)\n",
    "print(f\"Raw shapes: {[b[1] for b in band_counts[:5]]}\")\n",
    "print(f\"Trimmed bands: {band_counts[:5]}\")\n",
    "print(f\"Unique trimmed B: {sorted(unique_bands)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b38c8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 54 patches\n",
      "PCA: 55,296 pixels × 101 bands\n",
      "Variance: 100.0% (30 comps)\n",
      "PCA ready!!! 101→30 bands\n"
     ]
    }
   ],
   "source": [
    "def fit_hs_pca_fixed(train_df, target_ch=101, n_components=30, sample_frac=0.1):\n",
    "    all_pixels = []\n",
    "    hs_paths = train_df[\"hs\"].dropna().tolist()\n",
    "    \n",
    "    print(f\"Sampling {int(len(hs_paths)*sample_frac)} patches\")\n",
    "    for path in np.random.choice(hs_paths, size=int(len(hs_paths)*sample_frac), replace=False):\n",
    "        try:\n",
    "            arr = read_tiff_multiband(path)\n",
    "            B_full = arr.shape[2]\n",
    "            start, end = cfg.HS_DROP_FIRST, B_full - cfg.HS_DROP_LAST\n",
    "            \n",
    "            # Take exactly target_ch bands\n",
    "            actual_ch = min(end - start, target_ch)\n",
    "            arr_trim = arr[:,:,start:start+actual_ch]\n",
    "            \n",
    "            # Pad if needed\n",
    "            if arr_trim.shape[2] < target_ch:\n",
    "                pad_shape = (arr_trim.shape[0], arr_trim.shape[1], target_ch - arr_trim.shape[2])\n",
    "                arr_trim = np.pad(arr_trim, ((0,0),(0,0),(0,pad_shape[2])), mode='constant')\n",
    "            \n",
    "            # Flatten to pixels x bands (2D)\n",
    "            pixels_patch = arr_trim.reshape(-1, target_ch)\n",
    "            all_pixels.append(pixels_patch)\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    # Stack ALL pixels (2D: N_pixels × target_ch)\n",
    "    pixels = np.vstack(all_pixels)\n",
    "    print(f\"PCA: {pixels.shape[0]:,} pixels × {pixels.shape[1]} bands\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    pixels_scaled = scaler.fit_transform(pixels)\n",
    "    \n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(pixels_scaled)\n",
    "    \n",
    "    print(f\"Variance: {pca.explained_variance_ratio_.sum():.1%} ({pca.n_components_} comps)\")\n",
    "    \n",
    "    joblib.dump({\"pca\": pca, \"scaler\": scaler, \"target_ch\": target_ch}, \n",
    "                f\"{cfg.OUT_DIR}/hs_pca.pkl\")\n",
    "    return pca, scaler, target_ch\n",
    "\n",
    "pca, scaler, hs_pca_ch = fit_hs_pca_fixed(df_tr, target_ch=101, n_components=30)\n",
    "print(f\"PCA ready!!! 101→{pca.n_components_} bands\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb78528",
   "metadata": {},
   "source": [
    "## Int PCA w/ the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14bd8336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HS Dataset: raw 101 → PCA 30 bands\n",
      "Testing PCA dataset...\n",
      "PCA Sample shapes:\n",
      "  RGB: torch.Size([3, 224, 224])  mask: 1.0\n",
      "  MS:  torch.Size([5, 224, 224])  mask: 1.0\n",
      "  HS:  torch.Size([30, 224, 224])  mask: 1.0\n"
     ]
    }
   ],
   "source": [
    "# LOAD PCA\n",
    "pca_data = joblib.load(f\"{cfg.OUT_DIR}/hs_pca.pkl\")\n",
    "HS_PCA = pca_data[\"pca\"]\n",
    "HS_SCALER = pca_data[\"scaler\"] \n",
    "HS_PCA_CH = HS_PCA.n_components_  # 30\n",
    "print(f\"HS Dataset: raw {pca_data['target_ch']} → PCA {HS_PCA_CH} bands\")\n",
    "\n",
    "# def apply_hs_pca(x_hs: torch.Tensor) -> torch.Tensor:\n",
    "#     \"\"\"Apply scaler + PCA to HS tensor (C,H,W) → (pca_ch,H,W)\"\"\"\n",
    "#     # (C,H,W) → (H*W,C) → scale → PCA → (H*W,pca_ch) → (pca_ch,H,W)\n",
    "#     B, H, W = x_hs.shape\n",
    "#     pixels = x_hs.permute(1,2,0).reshape(-1, B).numpy()  # H*W x C\n",
    "    \n",
    "#     pixels_scaled = HS_SCALER.transform(pixels)\n",
    "#     pixels_pca = HS_PCA.transform(pixels_scaled)\n",
    "    \n",
    "#     return torch.from_numpy(pixels_pca.T).reshape(HS_PCA_CH, H, W).float()\n",
    "\n",
    "def apply_hs_pca(x_hs: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Apply scaler + PCA: truncate to 101 → scale → PCA → 30\"\"\"\n",
    "    B, H, W = x_hs.shape\n",
    "    \n",
    "    # FIXED: Truncate to exactly 101 bands (PCA input_ch)\n",
    "    x_hs_trunc = x_hs[:101]  # take first 101\n",
    "    \n",
    "    pixels = x_hs_trunc.permute(1,2,0).reshape(-1, 101).numpy()\n",
    "    \n",
    "    pixels_scaled = HS_SCALER.transform(pixels)\n",
    "    pixels_pca = HS_PCA.transform(pixels_scaled)\n",
    "    \n",
    "    return torch.from_numpy(pixels_pca.T).reshape(HS_PCA_CH, H, W).float()\n",
    "\n",
    "\n",
    "class WheatPCADataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, cfg, hs_pca_ch, train=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.cfg = cfg\n",
    "        self.hs_pca_ch = hs_pca_ch\n",
    "        self.train = train\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.df)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        x_rgb = x_ms = x_hs = None\n",
    "        mask = torch.zeros(3)\n",
    "        \n",
    "        # RGB (unchanged)\n",
    "        if self.cfg.USE_RGB and row.get(\"rgb\"):\n",
    "            x_rgb = read_rgb(row[\"rgb\"])\n",
    "            x_rgb = resize_tensor(x_rgb, self.cfg.IMG_SIZE)\n",
    "            mask[0] = 1.0\n",
    "        \n",
    "        # MS (unchanged)  \n",
    "        if self.cfg.USE_MS and row.get(\"ms\"):\n",
    "            arr = read_tiff_multiband(row[\"ms\"])\n",
    "            arr = (arr - arr.min(axis=(0,1), keepdims=True)) / (arr.max(axis=(0,1), keepdims=True) + 1e-6)\n",
    "            x_ms = torch.from_numpy(arr).permute(2,0,1).float()\n",
    "            x_ms = resize_tensor(x_ms, self.cfg.IMG_SIZE)\n",
    "            mask[1] = 1.0\n",
    "        \n",
    "        # HS + PCA! \n",
    "        if self.cfg.USE_HS and row.get(\"hs\"):\n",
    "            x_hs_raw = read_hs(row[\"hs\"], self.cfg.HS_DROP_FIRST, self.cfg.HS_DROP_LAST)\n",
    "            x_hs = apply_hs_pca(resize_tensor(x_hs_raw, self.cfg.IMG_SIZE))\n",
    "            mask[2] = 1.0\n",
    "        \n",
    "        # Pad missing\n",
    "        if x_rgb is None: x_rgb = torch.zeros(3, self.cfg.IMG_SIZE, self.cfg.IMG_SIZE)\n",
    "        if x_ms  is None: x_ms  = torch.zeros(5, self.cfg.IMG_SIZE, self.cfg.IMG_SIZE)\n",
    "        if x_hs  is None: x_hs  = torch.zeros(self.hs_pca_ch, self.cfg.IMG_SIZE, self.cfg.IMG_SIZE)\n",
    "        \n",
    "        out = {\"id\": row[\"base_id\"], \"rgb\": x_rgb, \"ms\": x_ms, \"hs\": x_hs, \"mask\": mask}\n",
    "        if \"label\" in row:\n",
    "            out[\"y\"] = torch.tensor(LBL2ID[row[\"label\"]])\n",
    "        return out\n",
    "\n",
    "# TEST PCA DATASET\n",
    "print(\"Testing PCA dataset...\")\n",
    "ds_pca_tr = WheatPCADataset(df_tr, cfg, HS_PCA_CH, train=True)\n",
    "sample_pca = ds_pca_tr[0]\n",
    "\n",
    "print(f\"PCA Sample shapes:\")\n",
    "print(f\"  RGB: {sample_pca['rgb'].shape}  mask: {sample_pca['mask'][0]:.1f}\")\n",
    "print(f\"  MS:  {sample_pca['ms'].shape}  mask: {sample_pca['mask'][1]:.1f}\")\n",
    "print(f\"  HS:  {sample_pca['hs'].shape}  mask: {sample_pca['mask'][2]:.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45033b94",
   "metadata": {},
   "source": [
    "## update model + train 1 epoch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1bcdc488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model updated for PCA HS: 30 input channels\n"
     ]
    }
   ],
   "source": [
    "model = MultiModalNet(cfg, HS_PCA_CH).to(device)  # 30 HS input channels!\n",
    "print(f\"Model updated for PCA HS: {HS_PCA_CH} input channels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55b1dcd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ResNet50 + PCA(30)\n",
      "  Params: 25,368,611\n",
      "Dataloaders: train=68 batches, val=8 batches\n"
     ]
    }
   ],
   "source": [
    "model = MultiModalNet(cfg, HS_PCA_CH).to(device)  # Uses 30 HS chans\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.LR, weight_decay=cfg.WD)\n",
    "\n",
    "print(f\"Model: ResNet50 + PCA(30)\")\n",
    "print(f\"  Params: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Dataloaders (small for CPU test)\n",
    "ds_tr_pca = WheatPCADataset(df_tr, cfg, HS_PCA_CH, train=True)\n",
    "ds_va_pca = WheatPCADataset(df_va_holdout, cfg, HS_PCA_CH, train=False)\n",
    "\n",
    "dl_tr = DataLoader(ds_tr_pca, batch_size=cfg.BATCH_SIZE, shuffle=True, \n",
    "                  num_workers=0, pin_memory=False)  # ← workers=0, no pin\n",
    "dl_va = DataLoader(ds_va_pca, batch_size=cfg.BATCH_SIZE, shuffle=False, \n",
    "                  num_workers=0, pin_memory=False)\n",
    "\n",
    "print(f\"Dataloaders: train={len(dl_tr)} batches, val={len(dl_va)} batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e7baea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1100; acc: 0.125\n"
     ]
    }
   ],
   "source": [
    "def train_one_batch(model, batch, optimizer, device):\n",
    "    rgb = batch[\"rgb\"].to(device)\n",
    "    ms  = batch[\"ms\"].to(device)  \n",
    "    hs  = batch[\"hs\"].to(device)\n",
    "    mask = batch[\"mask\"].to(device)\n",
    "    y = batch[\"y\"].to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    logits = model(rgb, ms, hs, mask)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    acc = (logits.argmax(1) == y).float().mean().item()\n",
    "    return loss.item(), acc\n",
    "\n",
    "# TEST\n",
    "model.train()\n",
    "batch = next(iter(dl_tr))\n",
    "loss, acc = train_one_batch(model, batch, optimizer, device)\n",
    "print(f\"Loss: {loss:.4f}; acc: {acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272768d9",
   "metadata": {},
   "source": [
    "# Proper train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6ec40ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        rgb, ms, hs, mask, y = [batch[k].to(device) for k in [\"rgb\",\"ms\",\"hs\",\"mask\",\"y\"]]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(rgb, ms, hs, mask)  # no autocast on CPU\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # gradient clipping\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * y.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    conf = torch.zeros(3, 3, dtype=torch.long)\n",
    "\n",
    "    for batch in loader:\n",
    "        rgb  = batch[\"rgb\"].to(device)\n",
    "        ms   = batch[\"ms\"].to(device)\n",
    "        hs   = batch[\"hs\"].to(device)\n",
    "        mask = batch[\"mask\"].to(device)\n",
    "        y    = batch[\"y\"].to(device)\n",
    "\n",
    "        logits = model(rgb, ms, hs, mask)\n",
    "        pred = logits.argmax(1)\n",
    "\n",
    "        total += y.size(0)\n",
    "        correct += (pred == y).sum().item()\n",
    "\n",
    "        yt = y.cpu().numpy()\n",
    "        yp = pred.cpu().numpy()\n",
    "        for t, p in zip(yt, yp):\n",
    "            conf[t, p] += 1\n",
    "\n",
    "    acc = correct / max(1, total)\n",
    "\n",
    "    f1s = []\n",
    "    for c in range(3):\n",
    "        tp = conf[c, c].item()\n",
    "        fp = int(conf[:, c].sum().item() - tp)\n",
    "        fn = int(conf[c, :].sum().item() - tp)\n",
    "        prec = tp / max(1, (tp + fp))\n",
    "        rec  = tp / max(1, (tp + fn))\n",
    "        f1 = 0.0 if (prec + rec) == 0 else (2 * prec * rec / (prec + rec))\n",
    "        f1s.append(f1)\n",
    "\n",
    "    return {\"acc\": acc, \"macro_f1\": sum(f1s) / 3.0}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e143b91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ResNet50 + PCA HS\n",
      "E 1: loss=0.9186 | acc= 33.3% | F1= 16.7%\n",
      "NEW BEST F1:  16.7%\n",
      "E 2: loss=0.8633 | acc= 33.3% | F1= 16.7%\n",
      "E 3: loss=0.8163 | acc= 33.3% | F1= 16.7%\n",
      "E 4: loss=0.7422 | acc= 33.3% | F1= 16.7%\n",
      "E 5: loss=0.6335 | acc= 33.3% | F1= 16.7%\n",
      "E 6: loss=0.6573 | acc= 33.3% | F1= 16.7%\n",
      "Early stop (no improve 5 epochs)\n",
      "\n",
      "FINAL best F1: 16.7%\n"
     ]
    }
   ],
   "source": [
    "print(\"Training ResNet50 + PCA HS\")\n",
    "best_f1, patience = 0.0, 5\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(1, 21):\n",
    "    tr_loss = train_epoch(model, dl_tr, optimizer, device)\n",
    "    metrics = evaluate(model, dl_va, device)\n",
    "\n",
    "    acc_pct = metrics[\"acc\"] * 100\n",
    "    f1_pct  = metrics[\"macro_f1\"] * 100\n",
    "\n",
    "    print(f\"E{epoch:2d}: loss={tr_loss:.4f} | acc={acc_pct:5.1f}% | F1={f1_pct:5.1f}%\")\n",
    "\n",
    "    if metrics[\"macro_f1\"] > best_f1:\n",
    "        best_f1 = metrics[\"macro_f1\"]\n",
    "        torch.save(model.state_dict(), f\"{cfg.OUT_DIR}/trial2_best.pt\")\n",
    "        no_improve = 0\n",
    "        print(f\"NEW BEST F1: {f1_pct:5.1f}%\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\"Early stop (no improve {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "print(f\"\\nFINAL best F1: {best_f1*100:.1f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdap",
   "language": "python",
   "name": "uv-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
